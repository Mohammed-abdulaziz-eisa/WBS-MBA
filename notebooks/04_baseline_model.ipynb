{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11345920,"sourceType":"datasetVersion","datasetId":7099154}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:28.532947Z","iopub.execute_input":"2025-04-10T00:13:28.533221Z","iopub.status.idle":"2025-04-10T00:13:30.619809Z","shell.execute_reply.started":"2025-04-10T00:13:28.533194Z","shell.execute_reply":"2025-04-10T00:13:30.618961Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1: Initial Setup and Data Loading","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.impute import SimpleImputer\nimport gc  # For memory management\n\n# Load data with reduced memory usage\ndef reduce_mem_usage(df):\n    \"\"\"Iterate through dataframe columns and reduce memory usage\"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Initial memory usage: {start_mem:.2f} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n            \n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n    return df\n\n# Load data\ndf_lct_dl = pd.read_csv('/kaggle/input/model-data/curr_lct_dl.csv')\ndf_lct_dl = reduce_mem_usage(df_lct_dl)\n\n# Remove duplicates\nprint(f\"Initial shape: {df_lct_dl.shape}\")\ndf_lct_dl.drop_duplicates(inplace=True)\nprint(f\"After removing duplicates: {df_lct_dl.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:30.621465Z","iopub.execute_input":"2025-04-10T00:13:30.621826Z","iopub.status.idle":"2025-04-10T00:13:38.457558Z","shell.execute_reply.started":"2025-04-10T00:13:30.621804Z","shell.execute_reply":"2025-04-10T00:13:38.456735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This is the most likely correct path\ndf_lct_dl = pd.read_csv('/kaggle/input/model-data/curr_lct_dl.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:38.458375Z","iopub.execute_input":"2025-04-10T00:13:38.458675Z","iopub.status.idle":"2025-04-10T00:13:40.747034Z","shell.execute_reply.started":"2025-04-10T00:13:38.458649Z","shell.execute_reply":"2025-04-10T00:13:40.746249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:40.747948Z","iopub.execute_input":"2025-04-10T00:13:40.748238Z","iopub.status.idle":"2025-04-10T00:13:40.775737Z","shell.execute_reply.started":"2025-04-10T00:13:40.748210Z","shell.execute_reply":"2025-04-10T00:13:40.774859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:40.777460Z","iopub.execute_input":"2025-04-10T00:13:40.777752Z","iopub.status.idle":"2025-04-10T00:13:41.017427Z","shell.execute_reply.started":"2025-04-10T00:13:40.777732Z","shell.execute_reply":"2025-04-10T00:13:41.016758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl['error_code'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:41.018227Z","iopub.execute_input":"2025-04-10T00:13:41.018498Z","iopub.status.idle":"2025-04-10T00:13:41.073144Z","shell.execute_reply.started":"2025-04-10T00:13:41.018474Z","shell.execute_reply":"2025-04-10T00:13:41.072470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"error_code_distribution = df_lct_dl['error_code'].value_counts()\nprint(error_code_distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:41.073924Z","iopub.execute_input":"2025-04-10T00:13:41.074210Z","iopub.status.idle":"2025-04-10T00:13:41.155736Z","shell.execute_reply.started":"2025-04-10T00:13:41.074183Z","shell.execute_reply":"2025-04-10T00:13:41.154924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:41.156529Z","iopub.execute_input":"2025-04-10T00:13:41.156746Z","iopub.status.idle":"2025-04-10T00:13:41.449109Z","shell.execute_reply.started":"2025-04-10T00:13:41.156729Z","shell.execute_reply":"2025-04-10T00:13:41.448350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl[['bytes_sec', 'duration', 'packets_received', 'packets_sent']].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:41.449853Z","iopub.execute_input":"2025-04-10T00:13:41.450131Z","iopub.status.idle":"2025-04-10T00:13:41.567925Z","shell.execute_reply.started":"2025-04-10T00:13:41.450107Z","shell.execute_reply":"2025-04-10T00:13:41.567188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl.duplicated().value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:41.568831Z","iopub.execute_input":"2025-04-10T00:13:41.569128Z","iopub.status.idle":"2025-04-10T00:13:42.319207Z","shell.execute_reply.started":"2025-04-10T00:13:41.569107Z","shell.execute_reply":"2025-04-10T00:13:42.318340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_lct_dl.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:13:42.320037Z","iopub.execute_input":"2025-04-10T00:13:42.320294Z","iopub.status.idle":"2025-04-10T00:13:42.547670Z","shell.execute_reply.started":"2025-04-10T00:13:42.320255Z","shell.execute_reply":"2025-04-10T00:13:42.546976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Error Code Distribution\n\n- **NO_ERROR**: 866,807 (99.7%)\n\n- **TCP_CONNECT**: 2,336 (0.27%)\n\n- **ZERO_SENT_SERVER**: 382 (0.04%)\n\n- **RCV_FAILED**: 175 (0.02%)\n\n- **TCP_RECV**: 1 (extremely rare)\n\n## Data Characteristics\n\n- **Missing Values**: None\n\n- **Duplicate Rows**: 246 (can be removed)\n\n- **Class Imbalance**: Extremely imbalanced classes (NO_ERROR dominates)\n\n## Numerical Features\n\n- **Variation**: Numerical features exhibit good variation.\n\n- **Predictive Potential**: `bytes_sec` shows high variability and could be highly predictive.","metadata":{}},{"cell_type":"markdown","source":"# 2: Feature Engineering and Preprocessing","metadata":{}},{"cell_type":"code","source":"# Extract features from datetime\ndf_lct_dl['dtime'] = pd.to_datetime(df_lct_dl['dtime'])\ndf_lct_dl['hour'] = df_lct_dl['dtime'].dt.hour.astype(np.int8)\ndf_lct_dl['day_of_week'] = df_lct_dl['dtime'].dt.dayofweek.astype(np.int8)\ndf_lct_dl['day_of_month'] = df_lct_dl['dtime'].dt.day.astype(np.int8)\ndf_lct_dl['month'] = df_lct_dl['dtime'].dt.month.astype(np.int8)\n\n# Drop original datetime column\ndf_lct_dl.drop('dtime', axis=1, inplace=True)\n\n# Assuming 'error_code' is our target - if not, replace with your actual target\n# If target is binary, we'll need to encode it\nif 'error_code' in df_lct_dl.columns:\n    df_lct_dl['error_code'] = df_lct_dl['error_code'].apply(lambda x: 0 if x == 'NO_ERROR' else 1).astype(np.int8)\n    target = 'error_code'\nelse:\n    # Replace with your actual target column\n    target = 'target_column'  \n\n# Separate features and target\nX = df_lct_dl.drop(target, axis=1)\ny = df_lct_dl[target]\n\n# Free up memory\ndel df_lct_dl\ngc.collect()\n\n# Split data - stratify to maintain class distribution\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Identify categorical and numerical columns\ncategorical_cols = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\nnumerical_cols = X_train.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns.tolist()\n\nprint(f\"Categorical columns: {categorical_cols}\")\nprint(f\"Numerical columns: {numerical_cols}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:14:19.768903Z","iopub.execute_input":"2025-04-10T00:14:19.769706Z","iopub.status.idle":"2025-04-10T00:14:21.338177Z","shell.execute_reply.started":"2025-04-10T00:14:19.769678Z","shell.execute_reply":"2025-04-10T00:14:21.337461Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3: Create Preprocessing Pipeline with Memory Efficiency","metadata":{}},{"cell_type":"code","source":"# Numerical pipeline with memory-efficient scaler\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical pipeline with low-memory one-hot encoding\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=True, dtype=np.int8))\n])\n\n# Full preprocessing pipeline\npreprocessor = ColumnTransformer([\n    ('num', num_pipeline, numerical_cols),\n    ('cat', cat_pipeline, categorical_cols)\n], sparse_threshold=0.3)  # Helps with memory for large datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:14:30.153059Z","iopub.execute_input":"2025-04-10T00:14:30.153362Z","iopub.status.idle":"2025-04-10T00:14:30.158941Z","shell.execute_reply.started":"2025-04-10T00:14:30.153343Z","shell.execute_reply":"2025-04-10T00:14:30.158082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4: Build Models with Feature Selection","metadata":{}},{"cell_type":"code","source":"# SVM Pipeline with reduced memory footprint\nsvm_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('feature_selector', SelectKBest(f_classif, k=20)),  # Select top 20 features\n    ('classifier', SVC(\n        kernel='rbf', \n        class_weight='balanced', \n        probability=True, \n        random_state=42,\n        gamma='scale',  # Helps with memory for large datasets\n        cache_size=500  # Limit cache size\n    ))\n])\n\n# Random Forest Pipeline with reduced memory\nrf_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('feature_selector', SelectKBest(f_classif, k=20)),  # Select top 20 features\n    ('classifier', RandomForestClassifier(\n        n_estimators=100,\n        class_weight='balanced',\n        random_state=42,\n        max_depth=10,  # Limit depth to prevent memory issues\n        n_jobs=-1  # Use all cores\n    ))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:14:37.468667Z","iopub.execute_input":"2025-04-10T00:14:37.468956Z","iopub.status.idle":"2025-04-10T00:14:37.474953Z","shell.execute_reply.started":"2025-04-10T00:14:37.468931Z","shell.execute_reply":"2025-04-10T00:14:37.474145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5: Train and Evaluate Models with Memory Management","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(pipeline, X_train, y_train, X_test, y_test, model_name):\n    print(f\"\\nTraining {model_name}...\")\n    \n    # Fit the model\n    pipeline.fit(X_train, y_train)\n    \n    # Predict and evaluate\n    y_pred = pipeline.predict(X_test)\n    print(f\"\\n{model_name} Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    \n    # Clear memory\n    gc.collect()\n    return pipeline\n\n# Train SVM (may be memory intensive)\ntry:\n    svm_model = train_and_evaluate(svm_pipeline, X_train, y_train, X_test, y_test, \"SVM\")\nexcept MemoryError:\n    print(\"SVM training failed due to memory constraints. Trying with smaller sample...\")\n    # Try with smaller sample if memory is an issue\n    X_train_sample, _, y_train_sample, _ = train_test_split(\n        X_train, y_train, train_size=0.5, random_state=42, stratify=y_train)\n    svm_model = train_and_evaluate(svm_pipeline, X_train_sample, y_train_sample, X_test, y_test, \"SVM (50% sample)\")\n\n# Train Random Forest (generally more memory-friendly)\nrf_model = train_and_evaluate(rf_pipeline, X_train, y_train, X_test, y_test, \"Random Forest\")\n\n# Compare model performance\nprint(\"\\nModel training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:14:49.424297Z","iopub.execute_input":"2025-04-10T00:14:49.424886Z","iopub.status.idle":"2025-04-10T00:17:48.733249Z","shell.execute_reply.started":"2025-04-10T00:14:49.424863Z","shell.execute_reply":"2025-04-10T00:17:48.732442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6: Deployment Preparation","metadata":{}},{"cell_type":"code","source":"import joblib\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a custom transformer for datetime features\nclass DateTimeTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        if 'dtime' in X.columns:\n            X['dtime'] = pd.to_datetime(X['dtime'])\n            X['hour'] = X['dtime'].dt.hour.astype(np.int8)\n            X['day_of_week'] = X['dtime'].dt.dayofweek.astype(np.int8)\n            X['day_of_month'] = X['dtime'].dt.day.astype(np.int8)\n            X['month'] = X['dtime'].dt.month.astype(np.int8)\n            X.drop('dtime', axis=1, inplace=True)\n        return X\n\n# Create a final pipeline that includes datetime processing\nfinal_pipeline = Pipeline([\n    ('datetime_processor', DateTimeTransformer()),\n    ('preprocessor', preprocessor),\n    ('feature_selector', SelectKBest(f_classif, k=20)),\n    ('classifier', RandomForestClassifier(\n        n_estimators=100,\n        class_weight='balanced',\n        random_state=42,\n        max_depth=10,\n        n_jobs=-1\n    ))\n])\n\n# Retrain on full data if needed\nfinal_pipeline.fit(X, y)\n\n# Save the model\njoblib.dump(final_pipeline, 'network_error_pipeline.pkl', compress=3)  # High compression to save space\n\nprint(\"Pipeline saved for deployment!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:18:29.073259Z","iopub.execute_input":"2025-04-10T00:18:29.074016Z","iopub.status.idle":"2025-04-10T00:18:48.237407Z","shell.execute_reply.started":"2025-04-10T00:18:29.073987Z","shell.execute_reply":"2025-04-10T00:18:48.236344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing Model Results","metadata":{}},{"cell_type":"markdown","source":"# 1. Confusion Matrices","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n# 1. First, let's create a more robust preprocessing pipeline\n# that will handle feature consistency automatically\n\n# Identify categorical and numerical columns (should be done before any splitting)\ncategorical_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()\nnumerical_cols = X.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns.tolist()\n\n# Create a robust preprocessing pipeline that maintains feature consistency\npreprocessor = make_column_transformer(\n    (make_pipeline(\n        SimpleImputer(strategy='median'),\n        StandardScaler()\n    ), numerical_cols),\n    (make_pipeline(\n        SimpleImputer(strategy='most_frequent'),\n        OneHotEncoder(handle_unknown='infrequent_if_exist', sparse=False)\n    ), categorical_cols),\n    remainder='drop'  # Drop any columns not explicitly transformed\n)\n\n# 2. Rebuild your models with this consistent preprocessing\nsvm_pipeline = make_pipeline(\n    preprocessor,\n    SelectKBest(f_classif, k=20),\n    SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n)\n\nrf_pipeline = make_pipeline(\n    preprocessor,\n    SelectKBest(f_classif, k=20),\n    RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, max_depth=10)\n)\n\n# 3. Retrain the models\nprint(\"Retraining models with consistent preprocessing...\")\nsvm_model = svm_pipeline.fit(X_train, y_train)\nrf_model = rf_pipeline.fit(X_train, y_train)\n\n# 4. Now the predictions should work correctly\ndef plot_confusion_matrix(y_true, y_pred, model_name, ax=None):\n    cm = confusion_matrix(y_true, y_pred)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n                xticklabels=['No Error', 'Error'],\n                yticklabels=['No Error', 'Error'])\n    ax.set_title(f'{model_name} Confusion Matrix')\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n\n# Get predictions from both models\ntry:\n    y_pred_svm = svm_model.predict(X_test)\n    y_pred_rf = rf_model.predict(X_test)\n    \n    # Create side-by-side confusion matrices\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n    plot_confusion_matrix(y_test, y_pred_svm, 'SVM', ax1)\n    plot_confusion_matrix(y_test, y_pred_rf, 'Random Forest', ax2)\n    plt.tight_layout()\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Error during prediction: {e}\")\n    print(\"\\nDebugging information:\")\n    print(f\"X_train shape: {X_train.shape}\")\n    print(f\"X_test shape: {X_test.shape}\")\n    print(\"Checking column differences:\")\n    train_cols = set(X_train.columns)\n    test_cols = set(X_test.columns)\n    print(f\"Columns in train but not test: {train_cols - test_cols}\")\n    print(f\"Columns in test but not train: {test_cols - train_cols}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:23:06.608605Z","iopub.execute_input":"2025-04-10T00:23:06.609506Z","iopub.status.idle":"2025-04-10T00:29:04.992851Z","shell.execute_reply.started":"2025-04-10T00:23:06.609474Z","shell.execute_reply":"2025-04-10T00:29:04.991990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. ROC Curves and AUC Scores","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import RocCurveDisplay\n\ndef plot_roc_curve(y_true, y_probs, model_name, ax=None):\n    fpr, tpr, _ = roc_curve(y_true, y_probs)\n    roc_auc = auc(fpr, tpr)\n    \n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n    \n    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, \n                   estimator_name=model_name).plot(ax=ax)\n    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n    ax.set_title(f'ROC Curve - {model_name}')\n    return roc_auc\n\n# Get probability predictions (we need to use predict_proba)\ny_proba_svm = svm_model.predict_proba(X_test)[:, 1]\ny_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n\n# Plot ROC curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\nauc_svm = plot_roc_curve(y_test, y_proba_svm, 'SVM', ax1)\nauc_rf = plot_roc_curve(y_test, y_proba_rf, 'Random Forest', ax2)\nplt.tight_layout()\nplt.show()\n\nprint(f\"SVM AUC: {auc_svm:.4f}\")\nprint(f\"Random Forest AUC: {auc_rf:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:30:02.822736Z","iopub.execute_input":"2025-04-10T00:30:02.823040Z","iopub.status.idle":"2025-04-10T00:30:07.516900Z","shell.execute_reply.started":"2025-04-10T00:30:02.823019Z","shell.execute_reply":"2025-04-10T00:30:07.516167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Precision-Recall Curves","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n\ndef plot_pr_curve(y_true, y_probs, model_name, ax=None):\n    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n    \n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n    \n    PrecisionRecallDisplay(precision=precision, recall=recall, \n                         estimator_name=model_name).plot(ax=ax)\n    ax.set_title(f'Precision-Recall Curve - {model_name}')\n    return auc(recall, precision)\n\n# Plot PR curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\npr_auc_svm = plot_pr_curve(y_test, y_proba_svm, 'SVM', ax1)\npr_auc_rf = plot_pr_curve(y_test, y_proba_rf, 'Random Forest', ax2)\nplt.tight_layout()\nplt.show()\n\nprint(f\"SVM PR AUC: {pr_auc_svm:.4f}\")\nprint(f\"Random Forest PR AUC: {pr_auc_rf:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T00:30:21.431655Z","iopub.execute_input":"2025-04-10T00:30:21.432309Z","iopub.status.idle":"2025-04-10T00:30:21.888127Z","shell.execute_reply.started":"2025-04-10T00:30:21.432262Z","shell.execute_reply":"2025-04-10T00:30:21.887391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}